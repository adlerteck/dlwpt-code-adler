{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.set_printoptions(edgeitems=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preperations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "class_names = ['airplane','automobile','bird','cat','deer',\n",
    "               'dog','frog','horse','ship','truck']\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "data_path = '../down/cifar-10/'\n",
    "cifar10 = datasets.CIFAR10(\n",
    "    data_path, train=True, download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))\n",
    "\n",
    "cifar10_val = datasets.CIFAR10(\n",
    "    data_path, train=False, download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))\n",
    "\n",
    "label_map = {0: 0, 2: 1}\n",
    "class_names = ['airplane', 'bird']\n",
    "cifar2 = [(img, label_map[label])\n",
    "          for img, label in cifar10\n",
    "          if label in [0, 2]]\n",
    "cifar2_val = [(img, label_map[label])\n",
    "              for img, label in cifar10_val\n",
    "              if label in [0, 2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cuda.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"Training on device {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device=device)  # <1>\n",
    "            labels = labels.to(device=device)\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{}: Epoch {:0>2d}, Training loss: {}'.format(\n",
    "                dt.datetime.now(), epoch, loss_train / len(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 8 * 8 * 8)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-27 21:24:04.136956: Epoch 01, Training loss: 0.5477554388107009\n",
      "2020-12-27 21:24:08.751513: Epoch 10, Training loss: 0.32743836882387756\n",
      "2020-12-27 21:24:13.595410: Epoch 20, Training loss: 0.2898802727840509\n",
      "2020-12-27 21:24:18.616266: Epoch 30, Training loss: 0.26690054926902623\n",
      "2020-12-27 21:24:23.630150: Epoch 40, Training loss: 0.24753980756185617\n",
      "2020-12-27 21:24:28.690816: Epoch 50, Training loss: 0.22892575791686964\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n",
    "\n",
    "model = Net().to(device=device)  # <1>\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 50,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.90\n",
      "Accuracy valid: 0.88\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)\n",
    "all_acc_dict = collections.OrderedDict()\n",
    "\n",
    "def validate(model, train_loader, val_loader):\n",
    "    accdict = {}\n",
    "    \n",
    "    for name, loader in [(\"train\", train_loader), (\"valid\", val_loader)]:\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in loader:\n",
    "                imgs = imgs.to(device=device)\n",
    "                labels = labels.to(device=device)\n",
    "                outputs = model(imgs)\n",
    "                _, predicted = torch.max(outputs, dim=1) # <1>\n",
    "                total += labels.shape[0]\n",
    "                correct += int((predicted == labels).sum())\n",
    "\n",
    "        accdict[name] = correct / total\n",
    "        print(\"Accuracy {}: {:.2f}\".format(name , correct / total))\n",
    "    \n",
    "    return accdict\n",
    "\n",
    "all_acc_dict[\"baseline\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = Net().to(device=device)\n",
    "loaded_model.load_state_dict(torch.load(data_path\n",
    "                                        + 'birds_vs_airplanes.pt',\n",
    "                                        map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Width Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetWidth(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 16, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(16 * 8 * 8, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 16 * 8 * 8)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-27 21:24:29.475265: Epoch 01, Training loss: 0.5418027360348185\n",
      "2020-12-27 21:24:33.912019: Epoch 10, Training loss: 0.3178438815721281\n",
      "2020-12-27 21:24:38.782683: Epoch 20, Training loss: 0.27610594679595557\n",
      "2020-12-27 21:24:43.781267: Epoch 30, Training loss: 0.2435441364053708\n",
      "2020-12-27 21:24:48.670979: Epoch 40, Training loss: 0.21469198186306437\n",
      "2020-12-27 21:24:53.483310: Epoch 50, Training loss: 0.18938815394404587\n",
      "\n",
      "Accuracy train: 0.90\n",
      "Accuracy valid: 0.88\n"
     ]
    }
   ],
   "source": [
    "model = NetWidth().to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 50,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "print()\n",
    "all_acc_dict[\"width32\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetWidth(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1 // 2)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-27 21:24:54.281269: Epoch 01, Training loss: 0.5381686266060848\n",
      "2020-12-27 21:24:58.659130: Epoch 10, Training loss: 0.3155044376090833\n",
      "2020-12-27 21:25:03.431840: Epoch 20, Training loss: 0.27211988788501473\n",
      "2020-12-27 21:25:08.202459: Epoch 30, Training loss: 0.23582879881000823\n",
      "2020-12-27 21:25:13.055274: Epoch 40, Training loss: 0.2069964709270532\n",
      "2020-12-27 21:25:17.940467: Epoch 50, Training loss: 0.18350385134197345\n",
      "\n",
      "Accuracy train: 0.90\n",
      "Accuracy valid: 0.88\n"
     ]
    }
   ],
   "source": [
    "model = NetWidth(n_chans1=32).to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 50,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "print()\n",
    "all_acc_dict[\"width32\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop_l2reg(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        \n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            l2_lambda = 0.001\n",
    "            l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())  # <1>\n",
    "            loss = loss + l2_lambda * l2_norm\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_train += loss.item()\n",
    "            \n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{}: Epoch {:0>2d}, Training loss: {}'.format(\n",
    "                dt.datetime.now(), epoch, loss_train / len(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-27 21:25:18.991214: Epoch 01, Training loss: 0.610285352559606\n",
      "2020-12-27 21:25:26.064784: Epoch 10, Training loss: 0.35443949927190305\n",
      "2020-12-27 21:25:33.868093: Epoch 20, Training loss: 0.32074438842238895\n",
      "2020-12-27 21:25:41.521030: Epoch 30, Training loss: 0.29852279441751495\n",
      "2020-12-27 21:25:49.468157: Epoch 40, Training loss: 0.2798329342132921\n",
      "2020-12-27 21:25:57.169697: Epoch 50, Training loss: 0.26363647401712503\n",
      "\n",
      "Accuracy train: 0.89\n",
      "Accuracy valid: 0.88\n"
     ]
    }
   ],
   "source": [
    "model = Net().to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop_l2reg(\n",
    "    n_epochs = 50,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "print()\n",
    "all_acc_dict[\"l2-reg\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetDropout(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv1_dropout = nn.Dropout2d(p=0.4)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, padding=1)\n",
    "        self.conv2_dropout = nn.Dropout2d(p=0.4)\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = self.conv1_dropout(out)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = self.conv2_dropout(out)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1 // 2)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-27 21:25:57.967642: Epoch 01, Training loss: 0.5619887855781871\n",
      "2020-12-27 21:26:02.788832: Epoch 10, Training loss: 0.37239494340814605\n",
      "2020-12-27 21:26:08.066624: Epoch 20, Training loss: 0.34669931413261756\n",
      "2020-12-27 21:26:13.465327: Epoch 30, Training loss: 0.33050744928372133\n",
      "2020-12-27 21:26:18.907841: Epoch 40, Training loss: 0.3166303885210851\n",
      "2020-12-27 21:26:24.208747: Epoch 50, Training loss: 0.29583633971062434\n",
      "\n",
      "Accuracy train: 0.87\n",
      "Accuracy valid: 0.86\n"
     ]
    }
   ],
   "source": [
    "model = NetDropout(n_chans1=32).to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 50,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "print()\n",
    "all_acc_dict[\"dropout\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model BatchNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetBatchNorm(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv1_batchnorm = nn.BatchNorm2d(num_features=n_chans1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, padding=1)\n",
    "        self.conv2_batchnorm = nn.BatchNorm2d(num_features=n_chans1 // 2)\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv1_batchnorm(self.conv1(x))\n",
    "        out = F.max_pool2d(torch.tanh(out), 2)\n",
    "        out = self.conv2_batchnorm(self.conv2(out))\n",
    "        out = F.max_pool2d(torch.tanh(out), 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1 // 2)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-27 21:26:25.152585: Epoch 01, Training loss: 0.4683299188021642\n",
      "2020-12-27 21:26:30.439051: Epoch 10, Training loss: 0.2674688303926189\n",
      "2020-12-27 21:26:36.372421: Epoch 20, Training loss: 0.20411465725131855\n",
      "2020-12-27 21:26:42.423878: Epoch 30, Training loss: 0.1535399579887937\n",
      "2020-12-27 21:26:48.323477: Epoch 40, Training loss: 0.10938137371069306\n",
      "2020-12-27 21:26:54.187020: Epoch 50, Training loss: 0.07617985909199639\n",
      "\n",
      "Accuracy train: 0.96\n",
      "Accuracy valid: 0.89\n"
     ]
    }
   ],
   "source": [
    "model = NetBatchNorm(n_chans1=32).to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 50,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "print()\n",
    "all_acc_dict[\"bat-norm\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model NetDepth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetDepth(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(n_chans1 // 2, n_chans1 // 2, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(4 * 4 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.relu(self.conv2(out)), 2)\n",
    "        out = F.max_pool2d(torch.relu(self.conv3(out)), 2)\n",
    "        out = out.view(-1, 4 * 4 * self.n_chans1 // 2)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-27 21:26:55.114559: Epoch 01, Training loss: 0.682493119103134\n",
      "2020-12-27 21:27:00.160384: Epoch 10, Training loss: 0.3407067284462558\n",
      "2020-12-27 21:27:05.961200: Epoch 20, Training loss: 0.2989848027373575\n",
      "2020-12-27 21:27:11.817830: Epoch 30, Training loss: 0.2684810319144255\n",
      "2020-12-27 21:27:17.506723: Epoch 40, Training loss: 0.2381876189807418\n",
      "2020-12-27 21:27:23.267889: Epoch 50, Training loss: 0.207651804159781\n",
      "\n",
      "Accuracy train: 0.88\n",
      "Accuracy valid: 0.87\n"
     ]
    }
   ],
   "source": [
    "model = NetDepth(n_chans1=32).to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 50,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "print()\n",
    "all_acc_dict[\"netdepth\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model NetRes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetRes(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(n_chans1 // 2, n_chans1 // 2, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(4 * 4 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.relu(self.conv2(out)), 2)\n",
    "        out1 = out\n",
    "        out = F.max_pool2d(torch.relu(self.conv3(out)) + out1, 2)\n",
    "        out = out.view(-1, 4 * 4 * self.n_chans1 // 2)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-27 21:27:24.158058: Epoch 01, Training loss: 0.6631431480881514\n",
      "2020-12-27 21:27:29.471358: Epoch 10, Training loss: 0.33765792789732574\n",
      "2020-12-27 21:27:35.219466: Epoch 20, Training loss: 0.29151152748211173\n",
      "2020-12-27 21:27:41.036748: Epoch 30, Training loss: 0.2556220719199272\n",
      "2020-12-27 21:27:46.781874: Epoch 40, Training loss: 0.22485817480049317\n",
      "2020-12-27 21:27:52.680579: Epoch 50, Training loss: 0.19745993694871855\n",
      "\n",
      "Accuracy train: 0.89\n",
      "Accuracy valid: 0.88\n"
     ]
    }
   ],
   "source": [
    "model = NetRes(n_chans1=32).to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 50,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "print()\n",
    "all_acc_dict[\"net-res\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model ResBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, n_chans):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(n_chans, n_chans, kernel_size=3, padding=1, bias=False)  # <1>\n",
    "        self.batch_norm = nn.BatchNorm2d(num_features=n_chans)\n",
    "        torch.nn.init.kaiming_normal_(self.conv.weight, nonlinearity='relu')  # <2>\n",
    "        torch.nn.init.constant_(self.batch_norm.weight, 0.5)\n",
    "        torch.nn.init.zeros_(self.batch_norm.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.batch_norm(out)\n",
    "        out = torch.relu(out)\n",
    "        return out + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetResDeep(nn.Module):\n",
    "    def __init__(self, n_chans1=32, n_blocks=10):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.resblocks = nn.Sequential(*(n_blocks * [ResBlock(n_chans=n_chans1)]))\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = self.resblocks(out)\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-27 21:28:02.198899: Epoch 01, Training loss: 1.4288039194170836\n",
      "2020-12-27 21:29:24.728259: Epoch 10, Training loss: 0.3004260933987654\n",
      "2020-12-27 21:30:55.652672: Epoch 20, Training loss: 0.2176339295069883\n",
      "2020-12-27 21:32:26.337681: Epoch 30, Training loss: 0.16135411401083516\n",
      "2020-12-27 21:33:56.816603: Epoch 40, Training loss: 0.17762522321123225\n",
      "2020-12-27 21:35:27.408995: Epoch 50, Training loss: 0.1403091109007787\n",
      "\n",
      "Accuracy train: 0.94\n",
      "Accuracy valid: 0.87\n"
     ]
    }
   ],
   "source": [
    "model = NetResDeep(n_chans1=32, n_blocks=100).to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=3e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 50,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "print()\n",
    "all_acc_dict[\"res-deep\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trn_acc = [v['train'] for k, v in all_acc_dict.items()]\n",
    "val_acc = [v['valid'] for k, v in all_acc_dict.items()]\n",
    "\n",
    "\n",
    "width =0.3\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.bar(np.arange(len(trn_acc)), trn_acc, width=width, label='train')\n",
    "plt.bar(np.arange(len(val_acc))+ width, val_acc, width=width, label='valid')\n",
    "plt.xticks(np.arange(len(val_acc))+ width/2, list(all_acc_dict.keys()), rotation=60)\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim(0.7, 1)\n",
    "# plt.savefig('accuracy_comparison.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "environment": {
   "name": "pytorch-gpu.1-6.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-6:m59"
  },
  "kernelspec": {
   "display_name": "Python 3.8 (conda:work)",
   "language": "python",
   "name": "python3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
