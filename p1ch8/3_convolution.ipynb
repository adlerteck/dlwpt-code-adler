{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.set_printoptions(edgeitems=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preperations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "class_names = ['airplane','automobile','bird','cat','deer',\n",
    "               'dog','frog','horse','ship','truck']\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "data_path = '../down/cifar-10/'\n",
    "cifar10 = datasets.CIFAR10(\n",
    "    data_path, train=True, download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))\n",
    "\n",
    "cifar10_val = datasets.CIFAR10(\n",
    "    data_path, train=False, download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))\n",
    "\n",
    "label_map = {0: 0, 2: 1}\n",
    "class_names = ['airplane', 'bird']\n",
    "cifar2 = [(img, label_map[label])\n",
    "          for img, label in cifar10\n",
    "          if label in [0, 2]]\n",
    "cifar2_val = [(img, label_map[label])\n",
    "              for img, label in cifar10_val\n",
    "              if label in [0, 2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cuda.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"Training on device {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device=device)  # <1>\n",
    "            labels = labels.to(device=device)\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{}: Epoch {:0>2d}, Training loss: {}'.format(\n",
    "                dt.datetime.now(), epoch, loss_train / len(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 8 * 8 * 8)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-01 00:51:20.042461: Epoch 01, Training loss: 0.5477554391903482\n",
      "2021-01-01 00:51:24.294476: Epoch 10, Training loss: 0.3274293661497201\n",
      "2021-01-01 00:51:29.007981: Epoch 20, Training loss: 0.2898688439730626\n",
      "2021-01-01 00:51:33.757601: Epoch 30, Training loss: 0.2668700110001169\n",
      "2021-01-01 00:51:38.483847: Epoch 40, Training loss: 0.24748397015841903\n",
      "2021-01-01 00:51:43.164377: Epoch 50, Training loss: 0.22887621995559923\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n",
    "\n",
    "model = Net().to(device=device)  # <1>\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 50,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.90\n",
      "Accuracy valid: 0.88\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)\n",
    "all_acc_dict = collections.OrderedDict()\n",
    "\n",
    "def validate(model, train_loader, val_loader):\n",
    "    accdict = {}\n",
    "    \n",
    "    for name, loader in [(\"train\", train_loader), (\"valid\", val_loader)]:\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in loader:\n",
    "                imgs = imgs.to(device=device)\n",
    "                labels = labels.to(device=device)\n",
    "                outputs = model(imgs)\n",
    "                _, predicted = torch.max(outputs, dim=1) # <1>\n",
    "                total += labels.shape[0]\n",
    "                correct += int((predicted == labels).sum())\n",
    "\n",
    "        accdict[name] = correct / total\n",
    "        print(\"Accuracy {}: {:.2f}\".format(name , correct / total))\n",
    "    \n",
    "    return accdict\n",
    "\n",
    "all_acc_dict[\"baseline\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = Net().to(device=device)\n",
    "loaded_model.load_state_dict(torch.load(data_path\n",
    "                                        + 'birds_vs_airplanes.pt',\n",
    "                                        map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Width Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetWidth(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 16, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(16 * 8 * 8, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 16 * 8 * 8)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-01 00:51:43.930982: Epoch 01, Training loss: 0.5418027343264051\n",
      "2021-01-01 00:51:48.066304: Epoch 10, Training loss: 0.3178378784922278\n",
      "2021-01-01 00:51:52.639108: Epoch 20, Training loss: 0.2760268520967216\n",
      "2021-01-01 00:51:57.237942: Epoch 30, Training loss: 0.2435669175283924\n",
      "2021-01-01 00:52:01.834481: Epoch 40, Training loss: 0.21475106653324358\n",
      "2021-01-01 00:52:06.419752: Epoch 50, Training loss: 0.18933348874947067\n",
      "\n",
      "Accuracy train: 0.90\n",
      "Accuracy valid: 0.88\n"
     ]
    }
   ],
   "source": [
    "model = NetWidth().to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 50,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "print()\n",
    "all_acc_dict[\"width32\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetWidth(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1 // 2)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-01 00:52:07.156488: Epoch 01, Training loss: 0.538168589210814\n",
      "2021-01-01 00:52:11.289672: Epoch 10, Training loss: 0.31550304876391294\n",
      "2021-01-01 00:52:15.861703: Epoch 20, Training loss: 0.27211676718323097\n",
      "2021-01-01 00:52:20.425598: Epoch 30, Training loss: 0.23581930311622135\n",
      "2021-01-01 00:52:24.971630: Epoch 40, Training loss: 0.20697227305477592\n",
      "2021-01-01 00:52:29.494837: Epoch 50, Training loss: 0.18345561571371782\n",
      "\n",
      "Accuracy train: 0.90\n",
      "Accuracy valid: 0.87\n"
     ]
    }
   ],
   "source": [
    "model = NetWidth(n_chans1=32).to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 50,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "print()\n",
    "all_acc_dict[\"width32\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop_l2reg(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        \n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            l2_lambda = 0.001\n",
    "            l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())  # <1>\n",
    "            loss = loss + l2_lambda * l2_norm\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_train += loss.item()\n",
    "            \n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{}: Epoch {:0>2d}, Training loss: {}'.format(\n",
    "                dt.datetime.now(), epoch, loss_train / len(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-01 00:52:30.488760: Epoch 01, Training loss: 0.6102853569255513\n",
      "2021-01-01 00:52:37.005492: Epoch 10, Training loss: 0.3544368408857637\n",
      "2021-01-01 00:52:44.164040: Epoch 20, Training loss: 0.32075700505523924\n",
      "2021-01-01 00:52:51.287472: Epoch 30, Training loss: 0.2985412694845989\n",
      "2021-01-01 00:52:58.455070: Epoch 40, Training loss: 0.2798359503222119\n",
      "2021-01-01 00:53:05.599610: Epoch 50, Training loss: 0.2636337302103164\n",
      "\n",
      "Accuracy train: 0.89\n",
      "Accuracy valid: 0.88\n"
     ]
    }
   ],
   "source": [
    "model = Net().to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop_l2reg(\n",
    "    n_epochs = 50,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "print()\n",
    "all_acc_dict[\"l2-reg\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetDropout(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv1_dropout = nn.Dropout2d(p=0.4)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, padding=1)\n",
    "        self.conv2_dropout = nn.Dropout2d(p=0.4)\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = self.conv1_dropout(out)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = self.conv2_dropout(out)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1 // 2)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-01 00:53:06.370521: Epoch 01, Training loss: 0.5619886961712199\n",
      "2021-01-01 00:53:10.794485: Epoch 10, Training loss: 0.37240118974713005\n",
      "2021-01-01 00:53:15.690811: Epoch 20, Training loss: 0.346705524879656\n",
      "2021-01-01 00:53:20.606953: Epoch 30, Training loss: 0.33050534271510545\n",
      "2021-01-01 00:53:25.579854: Epoch 40, Training loss: 0.31661834743372197\n",
      "2021-01-01 00:53:30.518296: Epoch 50, Training loss: 0.29580261800319524\n",
      "\n",
      "Accuracy train: 0.89\n",
      "Accuracy valid: 0.88\n"
     ]
    }
   ],
   "source": [
    "model = NetDropout(n_chans1=32).to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "model.train()\n",
    "training_loop(\n",
    "    n_epochs = 50,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "print()\n",
    "model.eval()\n",
    "all_acc_dict[\"dropout\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model BatchNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetBatchNorm(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv1_batchnorm = nn.BatchNorm2d(num_features=n_chans1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, padding=1)\n",
    "        self.conv2_batchnorm = nn.BatchNorm2d(num_features=n_chans1 // 2)\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv1_batchnorm(self.conv1(x))\n",
    "        out = F.max_pool2d(torch.tanh(out), 2)\n",
    "        out = self.conv2_batchnorm(self.conv2(out))\n",
    "        out = F.max_pool2d(torch.tanh(out), 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1 // 2)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-01 00:53:31.358906: Epoch 01, Training loss: 0.4683288301631903\n",
      "2021-01-01 00:53:36.373655: Epoch 10, Training loss: 0.26752165225660723\n",
      "2021-01-01 00:53:41.899595: Epoch 20, Training loss: 0.2042353250513411\n",
      "2021-01-01 00:53:47.468525: Epoch 30, Training loss: 0.15340587026943828\n",
      "2021-01-01 00:53:53.010266: Epoch 40, Training loss: 0.11011681565718287\n",
      "2021-01-01 00:53:58.535022: Epoch 50, Training loss: 0.0760115959984102\n",
      "\n",
      "Accuracy train: 0.96\n",
      "Accuracy valid: 0.89\n"
     ]
    }
   ],
   "source": [
    "model = NetBatchNorm(n_chans1=32).to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "model.train()\n",
    "training_loop(\n",
    "    n_epochs = 50,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "print()\n",
    "model.eval()\n",
    "all_acc_dict[\"bat-norm\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model NetDepth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetDepth(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(n_chans1 // 2, n_chans1 // 2, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(4 * 4 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.relu(self.conv2(out)), 2)\n",
    "        out = F.max_pool2d(torch.relu(self.conv3(out)), 2)\n",
    "        out = out.view(-1, 4 * 4 * self.n_chans1 // 2)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-01 00:53:59.384403: Epoch 01, Training loss: 0.682493119103134\n",
      "2021-01-01 00:54:04.229325: Epoch 10, Training loss: 0.3406400009515179\n",
      "2021-01-01 00:54:09.602175: Epoch 20, Training loss: 0.298995415970778\n",
      "2021-01-01 00:54:14.987030: Epoch 30, Training loss: 0.2685222531769686\n",
      "2021-01-01 00:54:20.386746: Epoch 40, Training loss: 0.23806965075860356\n",
      "2021-01-01 00:54:25.772401: Epoch 50, Training loss: 0.20774338574735982\n",
      "\n",
      "Accuracy train: 0.88\n",
      "Accuracy valid: 0.87\n"
     ]
    }
   ],
   "source": [
    "model = NetDepth(n_chans1=32).to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 50,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "print()\n",
    "all_acc_dict[\"netdepth\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model NetRes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetRes(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(n_chans1 // 2, n_chans1 // 2, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(4 * 4 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.relu(self.conv2(out)), 2)\n",
    "        out1 = out\n",
    "        out = F.max_pool2d(torch.relu(self.conv3(out)) + out1, 2)\n",
    "        out = out.view(-1, 4 * 4 * self.n_chans1 // 2)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-01 00:54:26.612262: Epoch 01, Training loss: 0.6631428303232618\n",
      "2021-01-01 00:54:31.428123: Epoch 10, Training loss: 0.3376867757861022\n",
      "2021-01-01 00:54:36.773742: Epoch 20, Training loss: 0.2913426273757485\n",
      "2021-01-01 00:54:42.064590: Epoch 30, Training loss: 0.2552363721618227\n",
      "2021-01-01 00:54:47.400679: Epoch 40, Training loss: 0.22443643824499884\n",
      "2021-01-01 00:54:52.734404: Epoch 50, Training loss: 0.19762664516070846\n",
      "\n",
      "Accuracy train: 0.89\n",
      "Accuracy valid: 0.88\n"
     ]
    }
   ],
   "source": [
    "model = NetRes(n_chans1=32).to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 50,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "print()\n",
    "all_acc_dict[\"net-res\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model ResBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, n_chans):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(n_chans, n_chans, kernel_size=3, padding=1, bias=False)  # <1>\n",
    "        self.batch_norm = nn.BatchNorm2d(num_features=n_chans)\n",
    "        torch.nn.init.kaiming_normal_(self.conv.weight, nonlinearity='relu')  # <2>\n",
    "        torch.nn.init.constant_(self.batch_norm.weight, 0.5)\n",
    "        torch.nn.init.zeros_(self.batch_norm.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.batch_norm(out)\n",
    "        out = torch.relu(out)\n",
    "        return out + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetResDeep(nn.Module):\n",
    "    def __init__(self, n_chans1=32, n_blocks=10):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.resblocks = nn.Sequential(*(n_blocks * [ResBlock(n_chans=n_chans1)]))\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = self.resblocks(out)\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-01 00:55:01.513465: Epoch 01, Training loss: 1.4269282908955956\n",
      "2021-01-01 00:56:17.368488: Epoch 10, Training loss: 0.29981718530320817\n",
      "2021-01-01 00:57:41.869147: Epoch 20, Training loss: 0.24775966978187014\n",
      "2021-01-01 00:59:06.319996: Epoch 30, Training loss: 0.18150420959113509\n",
      "2021-01-01 01:00:30.566463: Epoch 40, Training loss: 0.15666025042960977\n",
      "2021-01-01 01:01:55.036986: Epoch 50, Training loss: 0.10617878032955015\n",
      "\n",
      "Accuracy train: 0.95\n",
      "Accuracy valid: 0.87\n"
     ]
    }
   ],
   "source": [
    "model = NetResDeep(n_chans1=32, n_blocks=100).to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=3e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 50,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "print()\n",
    "all_acc_dict[\"res-deep\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trn_acc = [v['train'] for k, v in all_acc_dict.items()]\n",
    "val_acc = [v['valid'] for k, v in all_acc_dict.items()]\n",
    "\n",
    "\n",
    "width =0.3\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.bar(np.arange(len(trn_acc)), trn_acc, width=width, label='train')\n",
    "plt.bar(np.arange(len(val_acc))+ width, val_acc, width=width, label='valid')\n",
    "plt.xticks(np.arange(len(val_acc))+ width/2, list(all_acc_dict.keys()), rotation=60)\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim(0.7, 1)\n",
    "# plt.savefig('accuracy_comparison.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "environment": {
   "name": "pytorch-gpu.1-6.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-6:m59"
  },
  "kernelspec": {
   "display_name": "Python 3.8 (conda:work)",
   "language": "python",
   "name": "python3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
